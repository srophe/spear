name: Deploy to S3

on:
  push:
    branches:
      - 'main'

permissions:
  id-token: write
  contents: read

env:
  BUCKET_NAME: spear-front-end

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:

      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          fetch-depth: 0  # Full history for accurate diffs

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: ${{ secrets.AWS_SPEAR_ARN }}
          aws-region: ${{ secrets.AWS_REGION }}
          role-session-name: GitHub-OIDC-frontend-spear
          role-duration-seconds: 21600

      - name: Verify AWS CLI version
        run: aws --version

      # Step 3: Files to S3
      # This will add and overwrite files to the S3 bucket but not delete files in S3 that have been deleted in the repo
      - name: Sync files in S3
        if: success()
        run: |
          aws s3 sync ./ s3://spear-front-end --exact-timestamps --exclude "data/*" --exclude ".github/*"
        env:
          AWS_REGION: ${{ secrets.AWS_REGION }}
          AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID }} 
      
      # # Option 1: Upload only changed data files to S3
      # - name: Generate list of all changed files
      #   run: |
      #     git fetch origin main
      #     git diff --name-only origin/main..HEAD > changed_files.txt || true
      # - name: Upload changed data files in parallel
      #   run: |
      #     MAX_PARALLEL=8  # Adjust this depending on runner capacity
      
      #     # Count how many files were found
      #     total_files=$(grep -c '^' changed_data_files.txt || true)
      #     echo "‚úÖ Found $total_files changed data HTML file(s) to upload."
      
      #     if [ "$total_files" -eq 0 ]; then
      #       echo "‚ÑπÔ∏è No changed files to upload. Skipping."
      #       exit 0
      #     fi
      
      #     # Function to throttle concurrent jobs
      #     function wait_for_jobs() {
      #       local joblist=($(jobs -p))
      #       while [ ${#joblist[@]} -ge $MAX_PARALLEL ]; do
      #         sleep 1
      #         joblist=($(jobs -p))
      #       done
      #     }
      
      #     uploaded_count=0
      
      #     # Upload each file in background
      #     while read file; do
      #       if [ -f "$file" ]; then
      #         filename=$(basename "$file" .html)
      #         echo "üöÄ Uploading $file as s3://spear-front-end/$filename"
      #         (
      #           aws s3 cp "$file" "s3://spear-front-end/$filename" \
      #             --content-type text/html \
      #             --only-show-errors && \
      #             echo "$file uploaded" && uploaded_count=$((uploaded_count+1))
      #         ) &
      #         wait_for_jobs
      #       else
      #         echo "‚ö†Ô∏è Skipping missing file: $file"
      #       fi
      #     done < changed_data_files.txt
      
      #     wait  # Wait for all background uploads to finish
      
      #     echo "‚úÖ Upload complete. Attempted: $total_files file(s)."
      #     echo "‚úÖ Parallel uploads finished."
      #   env:
      #     AWS_REGION: ${{ secrets.AWS_REGION }}
      #     AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID }}

      - name: Invalidate CloudFront Cache
        run: |
          aws cloudfront create-invalidation \
            --distribution-id ${{ secrets.SPEAR_CLOUDFRONT_DISTRIBUTION_ID}} \
            --paths "/*"
        env:
          AWS_REGION: ${{ secrets.AWS_REGION }}
          AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID }}     
