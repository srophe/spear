name: Deploy to S3

on:
  push:
    branches:
      - 'main'  # Trigger the workflow on push to the main branch

permissions:
  id-token: write
  contents: read

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
    
      # Step 1: Check out the repository code
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          fetch-depth: 0  # Fetch the full history for accurate diff comparisons

      # Step 2: Configure AWS credentials
      - name: Configure AWS credentials from AWS account
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: ${{ secrets.AWS_SPEAR_ARN }}
          aws-region: ${{ secrets.AWS_REGION }}
          role-session-name: GitHub-OIDC-frontend-spear
          
      # Step 3: Files to S3
      # Option 1: Upload only changed data files to S3, sync the rest of the code base
      - name: Get list of changed files
        id: changes
        run: |
          git fetch origin main
          git diff --name-only origin/main...HEAD | grep '^data/.*\.html$' > changed_data_files.txt
          cat changed_files.txt
          
      # This will add files to the S3 bucket
      - name: Sync code files in S3
        if: success()
        run: |
          aws s3 sync ./ s3://spear-front-end \
            --delete \
            --exact-timestamps \
            --exclude "data/*" --exclude ".git/*" --exclude ".github/*" --exclude ".gitignore/*"
        env:
          AWS_REGION: ${{ secrets.AWS_REGION }}
          AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID }} 

      # Upload only changed files from data/ (stripped path & extension) (slow process)
      - name: Upload changed data files
        if: success()
        run: |
          while read file; do
            if [ -f "$file" ]; then
              filename=$(basename "$file" .html)
              echo "Uploading $file as $filename"
              aws s3 cp "$file" "s3://spear-front-end/$filename" \
                --content-type text/html \
                --only-show-errors
            fi
          done < changed_data_files.txt          
          
      - name: Invalidate CloudFront Cache
        run: |
          aws cloudfront create-invalidation \
            --distribution-id ${{ secrets.SPEAR_CLOUDFRONT_DISTRIBUTION_ID}} \
            --paths "/*"
        env:
          AWS_REGION: ${{ secrets.AWS_REGION }}
          AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID }} 
          
      # Option 2: Full (re)deployment

      # - name: Sync all files to S3
      #   run: |
      #     aws s3 sync ./ s3://spear-front-end --delete --exact-timestamps --exclude "data/*"
      #   env:
      #     AWS_REGION: ${{ secrets.AWS_REGION }}
      #     AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID }} 
      
      # - name: Upload changed data files in parallel
      #   run: |
      #     MAX_PARALLEL=8  # Adjust this depending on runner capacity
      
      #     # Function to track background jobs
      #     function wait_for_jobs() {
      #       local joblist=($(jobs -p))
      #       while [ ${#joblist[@]} -ge $MAX_PARALLEL ]; do
      #         sleep 1
      #         joblist=($(jobs -p))
      #       done
      #     }
      
      #     # Start parallel uploads
      #     while read file; do
      #       if [ -f "$file" ]; then
      #         filename=$(basename "$file" .html)
      #         echo "Uploading $file as $filename"
      #         aws s3 cp "$file" "s3://spear-front-end/$filename" \
      #           --content-type text/html \
      #           --only-show-errors &
      #         wait_for_jobs  
      #       fi
      #     done < changed_data_files.txt
      
      #     wait  # Wait for all background uploads to finish

      #   env:
      #     AWS_REGION: ${{ secrets.AWS_REGION }}
      #     AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID }}      
      
          
