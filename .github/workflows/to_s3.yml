name: Deploy to S3

on:
  push:
    branches:
      - 'main'  # Trigger the workflow on push to the main branch

permissions:
  id-token: write
  contents: read

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
    
      # Step 1: Check out the repository code
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          fetch-depth: 0  # Fetch the full history for accurate diff comparisons

      # Step 2: Configure AWS credentials
      - name: Configure AWS credentials from AWS account
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: ${{ secrets.AWS_SPEAR_ARN }}
          aws-region: ${{ secrets.AWS_REGION }}
          role-session-name: GitHub-OIDC-frontend-spear
          
      # Step 3: Files to S3
            # Option 1: Upload only changed files to S3
      - name: Get list of changed files
        id: changes
        run: |
          git fetch origin main
          # git diff --name-only HEAD^ HEAD > changed_files.txt
          git diff --name-only origin/main...HEAD > changed_files.txt
          grep '^data/.*\.html$' changed_files.txt > changed_data_files.txt || true
          cat changed_files.txt
          
      # This will add files to the S3 bucket and also delete files in S3 not in the repo
      - name: Sync files in S3
        if: success()
        run: |
          aws s3 sync ./ s3://spear-front-end --delete --exact-timestamps --exclude "data/*" --exclude ".github/*"
        env:
          AWS_REGION: ${{ secrets.AWS_REGION }}
          AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID }} 
          
      - name: Upload changed data files
        run: |
          if [ ! -s changed_data_files.txt ]; then
            echo "No changed HTML files in data/. Skipping upload."
            exit 0
          fi

          MAX_PARALLEL=4

          function wait_for_jobs() {
            local joblist=($(jobs -p))
            while [ ${#joblist[@]} -ge $MAX_PARALLEL ]; do
              sleep 1
              joblist=($(jobs -p))
            done
          }

          while read file; do
            if [ -f "$file" ]; then
              filename=$(basename "$file" .html)
              echo "Uploading $file as $filename"
              aws s3 cp "$file" "s3://spear-front-end/$filename" \
                --content-type text/html \
                --only-show-errors &
              wait_for_jobs
            fi
          done < changed_data_files.txt

          wait

        env:
          AWS_REGION: ${{ secrets.AWS_REGION }}
          AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID }}           

      # - name: Invalidate CloudFront Cache
      #   run: |
      #     aws cloudfront create-invalidation \
      #       --distribution-id ${{ secrets.SPEAR_CLOUDFRONT_DISTRIBUTION_ID}} \
      #       --paths "/*"
      #   env:
      #     AWS_REGION: ${{ secrets.AWS_REGION }}
      #     AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID }} 
          
      # Option 2: Full (re)deployment

      # - name: Sync all files to S3
      #   run: |
      #     aws s3 sync ./ s3://spear-front-end --delete --exact-timestamps --exclude "data/*"
      #   env:
      #     AWS_REGION: ${{ secrets.AWS_REGION }}
      #     AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID }} 
      # - name: Upload changed data files in parallel
      #   run: |
      #     MAX_PARALLEL=8  # Adjust this depending on runner capacity
      
      #     # Function to track background jobs
      #     function wait_for_jobs() {
      #       local joblist=($(jobs -p))
      #       while [ ${#joblist[@]} -ge $MAX_PARALLEL ]; do
      #         sleep 1
      #         joblist=($(jobs -p))
      #       done
      #     }
      
      #     # Start parallel uploads
      #     while read file; do
      #       if [ -f "$file" ]; then
      #         filename=$(basename "$file" .html)
      #         echo "Uploading $file as $filename"
      #         aws s3 cp "$file" "s3://spear-front-end/$filename" \
      #           --content-type text/html \
      #           --only-show-errors &
      #         wait_for_jobs  
      #       fi
      #     done < changed_data_files.txt
      
      #     wait  # Wait for all background uploads to finish

      #   env:
      #     AWS_REGION: ${{ secrets.AWS_REGION }}
      #     AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID }}  
